# -*- coding: utf-8 -*-
"""Q_1_vector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GnC-oE1H2VLD9ZAQ3g4k-l8qDR_hEcPi
"""

import os
import nltk
import copy
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import numpy as np
from collections import OrderedDict
import math
import operator
import gensim
import math
import random
import gensim

from google.colab import drive
drive.mount('/content/drive')

!cp -r "/content/drive/My Drive/GoogleNews-vectors-negative300.bin" .

!unzip Q4_DATASET.zip

dataset_path = 'Q4_DATASET'
stop_words = set(stopwords.words('english'))
directory = os.listdir(dataset_path)
model=gensim.models.KeyedVectors.load_word2vec_format('/content/GoogleNews-vectors-negative300.bin', binary=True)

def preprocess(line):
    tokenizer = nltk.RegexpTokenizer('\w+')
    tokens_list = tokenizer.tokenize(line)
    for tokens in tokens_list:
        if tokens in stop_words:
            tokens_list.remove(tokens)
    lemmatizer = WordNetLemmatizer()
    words_list=[]
    for tokens in tokens_list:
        words_list.append(lemmatizer.lemmatize(tokens))

    words_list = [element.lower() for element in words_list]
    return words_list

##################distance calculation####################
def calculate_distance(d1,d2):
    #print(d1)
    #print(d2)
    #dist=0
    #for i in range(0,len(d1)):
    #    dist+=abs(d1[i]-d2[i])**2
    return np.linalg.norm(np.asarray(d1)-np.asarray(d2))

def select_random_seeds(dataset,k):
    seed=[]
    index=[]
    #for i in range(0,k):
    #  index.append(random.randint(i*1000,1000*(i+1)))
    #print(index)
    index=np.random.choice(range(5000), k, replace=False)
    #index=[3099,4791,743,526,4884]
    for i in range(0,k):
        seed.append(dataset[index[i]])
    return seed

def assign_cluster(data,clusters):
    min=999999
    cluster=0
    for i in range(0,len(clusters)):
        dist=calculate_distance(data,clusters[i])
        if min>dist:
            min=dist
            cluster=i
    return cluster

def update_mean(cluster):
    final_cluster=[]
    for k in cluster.keys():
      final_cluster.append(np.mean(cluster[k],axis=0))
    #for k in cluster.keys():
    #    docs=cluster[k]
    #    val=[]
    #    for i in range(0,len(vocabs_list)):
    #        score=0
    #        for d in range(0,len(docs)):
    #            score+=docs[d][i]
    #        val.append(score)
    #    for v in range(0,len(val)):
    #        val[v]=val[v]/len(docs)
    #    final_cluster.append(val)
    #return final_cluster
    return final_cluster

###############idf calculation####################
def calculate_idf(dataset):
  vocab_idf=dict()
  for doc in dataset:
    for word in doc.keys():
      if word in vocab_idf.keys():
        vocab_idf[word]+=1
      else:
        vocab_idf[word]=1
  for k in vocab_idf.keys():
    vocab_idf[k]=len(dataset)/vocab_idf[k]
  return vocab_idf

def doc2vec(doc):
    vector=[]
    for word in doc.keys():
        try:
          vec=model[word]
          vec=[x*doc[word]*vocab_idf[word] for x in vec]
          vector.append(vec)
        except:
          continue
    vec = np.mean(vector,axis=0)
    return list(vec/np.linalg.norm(vec))

########purity calculation######################
def calculate_purity(cluster):
  val=0
  for k in cluster.keys():
    docs=cluster[k]
    docs=[math.floor(data.index(d)/1000) for d in docs]
    val+=docs.count(max(docs,key=docs.count))
    print(val)
  return val/5000

########purity calculation######################
def calculate_purity_1(cluster):
  val=0
  for k in cluster.keys():
    docs=cluster[k]
    docs=[label[str(d)] for d in docs]
    val+=docs.count(max(docs,key=docs.count))
    print(val)
  return val/5000

#################rss calculation####################
def calculate_rss(cluster,centroid):
  rss=0
  for k in cluster.keys():
    for doc in cluster[k]:
      rss+=calculate_distance(doc,centroid[k])**2
  return rss

################ari calculation #######################
def calculate_ari(cluster):
  ari=np.zeros((5,5))
  index=0
  for i in range(0,5):
    docs=cluster[i]
    class_representation=[math.floor(data.index(d)/1000) for d in docs]
    for j in range(0,5):
      ari[i][j]=class_representation.count(j)
      index+=(ari[i][j]*(ari[i][j]-1))/2
  a=np.sum(ari,axis=1)
  b=np.sum(ari,axis=0)
  exp1=0
  exp2=0
  for i in range(0,5):
    exp1+=(a[i]*(a[i]-1))/2
    exp2+=(b[i]*(b[i]-1))/2
  expected_sum=2*(exp1*exp2)/(5000*4999)
  return (index-expected_sum)/(0.5*(exp1+exp2)-expected_sum)

################ari calculation for bag of words #######################
def calculate_ari_1(cluster):
  ari=np.zeros((5,5))
  index=0
  for i in range(0,5):
    docs=cluster[i]
    class_representation=[label[str(d)] for d in docs]
    for j in range(0,5):
      ari[i][j]=class_representation.count(j)
      index+=(ari[i][j]*(ari[i][j]-1))/2
  a=np.sum(ari,axis=1)
  b=np.sum(ari,axis=0)
  exp1=0
  exp2=0
  for i in range(0,5):
    exp1+=(a[i]*(a[i]-1))/2
    exp2+=(b[i]*(b[i]-1))/2
  expected_sum=2*(exp1*exp2)/(5000*4999)
  return (index-expected_sum)/(0.5*(exp1+exp2)-expected_sum)

############################DATA PREPROCESSS########################3
label=-1
doc_id=0
flag=0
dataset=[]
dataset_label=[]
vocabs_list=[]
for folder in directory:
    files_list = os.listdir(dataset_path+'/'+folder)
    label+=1
    vocab_count=0
    doc_count=0
    for file in files_list:
        vocab_dict=dict()
        test_file=[]
        file = open(dataset_path+'/'+folder+'/'+file,'r',encoding="utf-8",errors='replace')
        file_data = file.readlines()
        fileName=file.name.split("/")[-2:]
        file_name=""
        file_name=file_name.join(fileName)
        for line in file_data:
            procesed_word_list = preprocess(line)
            for word in procesed_word_list:
                    if word in vocab_dict.keys():
                        vocab_dict[word]+=1
                    else:
                        if word not in vocabs_list:
                            vocabs_list.append(word)
                        vocab_dict[word]=1
        dataset.append(vocab_dict)
        dataset_label.append(label)
        doc_id+=1

############################vector formation##########################
data=[]
for d in dataset:
    vector=[]
    for v in vocabs_list:
        if v in d.keys():
            vector.append(d[v])
        else:
            vector.append(0)
    data.append(np.asarray(vector)/np.linalg.norm(vector))

#######################labelset for data in bag of word model###################
label=dict()
for i in range(0,len(data)):
  label[str(data[i])]=math.floor(i/1000)

#########################idf formation############################
vocab_idf=calculate_idf(dataset)

############################word to vec formation######################
data=[]
for doc in dataset:
  data.append(doc2vec(doc))

####################centeroid calculation###########################
k=5
centroid=select_random_seeds(data,k)

k=5
iteration=20
rss_list=[]
#centroid=select_random_seeds(data,k)
while(iteration>0):
    cluster=dict()
    c=0
    for d in data:
        cl=assign_cluster(d,centroid)
        if cl not in cluster.keys():
            temp=[]
            temp.append(d)
            cluster[cl]=temp
        else:
            cluster[cl].append(d)
        #if c%500==0:
        #  print(c)gd
        c+=1
    #for k in cluster.keys():
    #  print(k,len(cluster[k]))
    rss=calculate_rss(cluster,centroid)
    print(rss)
    rss_list.append(rss)
    centroid=update_mean(cluster)
    print("iteration: ",iteration)
    iteration-=1

for k in cluster.keys():
  print(k,len(cluster[k]))
print(rss_list)

np.shape(cluster[0][0])

######################evaluation matrices#####################
purity=calculate_purity(cluster)
print(purity)
rss=calculate_rss(cluster,centroid)
print(rss)
ari=calculate_ari(cluster)
print(ari)

data[0]